{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import word2vec\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the saved model and cleaned review data from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load('../data/300features_40minwords_10context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/labeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/testData.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleantrainreviews.txt', 'rb') as fp:\n",
    "    clean_train_reviews = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleantestreviews.txt', 'rb') as fp:\n",
    "    clean_test_reviews = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a start time, to see how long this takes, because clustering with KMeans can take a while. Setting $k$ equal to a fifth of the vocabulary size, which would give an average of 5 words per cluster. Wrapping the whole thing in an end time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means Clustering:  339.61628890037537 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "word_vectors = model.wv.vectors\n",
    "\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "kmeans = KMeans(n_clusters=int(num_clusters))\n",
    "\n",
    "idx = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "\n",
    "print(\"Time taken for K Means Clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.65"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "339 / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Word/Index dictionary, mapping each vocabulary word to a cluster number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_mapping = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a loop to print out the words in the first 10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['vs', 'versus']\n",
      "Cluster 1\n",
      "['command', 'troop', 'regiment']\n",
      "Cluster 2\n",
      "['unbelievable', 'unrealistic', 'unconvincing', 'implausible']\n",
      "Cluster 3\n",
      "['olivier', 'rupert', 'brett', 'scrooge', 'laurence', 'marlon', 'keanu', 'wilde', 'sleuth', 'fishburne', 'irons']\n",
      "Cluster 4\n",
      "['ingenious', 'offbeat', 'intricate', 'unconventional', 'oddball', 'eclectic', 'hitchcockian']\n",
      "Cluster 5\n",
      "['finishes']\n",
      "Cluster 6\n",
      "['privileged']\n",
      "Cluster 7\n",
      "['president', 'charge', 'prisoner', 'attorney', 'minister', 'senator', 'senior', 'warden', 'fugitive', 'convict', 'governor', 'pope', 'freed', 'parole', 'passport', 'probation']\n",
      "Cluster 8\n",
      "['racism', 'bashing', 'stereotyping', 'hiv', 'sexism', 'overwhelmingly', 'homophobia']\n",
      "Cluster 9\n",
      "['returns', 'returning', 'traveling', 'travels', 'traveled', 'guides']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    print('Cluster', cluster)\n",
    "    \n",
    "    words = []\n",
    "    for i in range(0, len(word_centroid_mapping.values())):\n",
    "        if(list(word_centroid_mapping.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_mapping.keys())[i])\n",
    "    \n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the clusters seem to make sense, like cluster 2 being military terms, and cluster 3 being all names, but some are just one-off clusters with only 1 word in them, like clusters 5 and 6 having 'finishes' and 'privileged', respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have clusters, also known as centroids, I can try out a \"bag-of-centroids\" model, which is essentially like Bag of Words, but uses clusters instead of just words. I start off by creating a custom function that will give me a numpy array for each review that has the number of features equal to the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_centroids(wordlist, word_centroid_mapping):\n",
    "\n",
    "    # The number of clusters is equal to the highest cluster index in the word/centroid map\n",
    "    num_centroids = max(word_centroid_mapping.values()) + 1\n",
    "\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_mapping:\n",
    "            index = word_centroid_mapping[word]\n",
    "            bag_centroids[index] += 1\n",
    "\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I use the custom function from above to creat the bags of centroids for the review training and tests sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((25000, int(num_clusters)), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = bag_of_centroids(review, word_centroid_mapping)\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros((25000, int(num_clusters)), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = bag_of_centroids(review, word_centroid_mapping)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the newly created bags of centroids to train a random forest model again to compare with the previous attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_centroids, train.sentiment, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84368"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the bag of centroids model did negligibly better than the previous bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
