{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the pickled sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sentences.txt', 'rb') as fp:\n",
    "    sentences = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795534"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some logs of my various parameters, initializing the Word2Vec model, and training on the sentences data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-17 13:19:03,094 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-05-17 13:19:03,099 : INFO : collecting all words and their counts\n",
      "2019-05-17 13:19:03,099 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-17 13:19:03,133 : INFO : PROGRESS: at sentence #10000, processed 225799 words, keeping 17773 word types\n",
      "2019-05-17 13:19:03,169 : INFO : PROGRESS: at sentence #20000, processed 451864 words, keeping 24940 word types\n",
      "2019-05-17 13:19:03,205 : INFO : PROGRESS: at sentence #30000, processed 671284 words, keeping 30022 word types\n",
      "2019-05-17 13:19:03,240 : INFO : PROGRESS: at sentence #40000, processed 897762 words, keeping 34337 word types\n",
      "2019-05-17 13:19:03,275 : INFO : PROGRESS: at sentence #50000, processed 1116945 words, keeping 37748 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-17 13:19:03,310 : INFO : PROGRESS: at sentence #60000, processed 1338366 words, keeping 40707 word types\n",
      "2019-05-17 13:19:03,345 : INFO : PROGRESS: at sentence #70000, processed 1561514 words, keeping 43317 word types\n",
      "2019-05-17 13:19:03,381 : INFO : PROGRESS: at sentence #80000, processed 1780803 words, keeping 45695 word types\n",
      "2019-05-17 13:19:03,416 : INFO : PROGRESS: at sentence #90000, processed 2004876 words, keeping 48112 word types\n",
      "2019-05-17 13:19:03,453 : INFO : PROGRESS: at sentence #100000, processed 2226855 words, keeping 50183 word types\n",
      "2019-05-17 13:19:03,488 : INFO : PROGRESS: at sentence #110000, processed 2446475 words, keeping 52056 word types\n",
      "2019-05-17 13:19:03,525 : INFO : PROGRESS: at sentence #120000, processed 2668648 words, keeping 54090 word types\n",
      "2019-05-17 13:19:03,561 : INFO : PROGRESS: at sentence #130000, processed 2894169 words, keeping 55819 word types\n",
      "2019-05-17 13:19:03,595 : INFO : PROGRESS: at sentence #140000, processed 3106859 words, keeping 57315 word types\n",
      "2019-05-17 13:19:03,632 : INFO : PROGRESS: at sentence #150000, processed 3332467 words, keeping 59023 word types\n",
      "2019-05-17 13:19:03,667 : INFO : PROGRESS: at sentence #160000, processed 3555121 words, keeping 60586 word types\n",
      "2019-05-17 13:19:03,703 : INFO : PROGRESS: at sentence #170000, processed 3778458 words, keeping 62044 word types\n",
      "2019-05-17 13:19:03,739 : INFO : PROGRESS: at sentence #180000, processed 3999008 words, keeping 63457 word types\n",
      "2019-05-17 13:19:03,776 : INFO : PROGRESS: at sentence #190000, processed 4224207 words, keeping 64756 word types\n",
      "2019-05-17 13:19:03,813 : INFO : PROGRESS: at sentence #200000, processed 4448298 words, keeping 66044 word types\n",
      "2019-05-17 13:19:03,850 : INFO : PROGRESS: at sentence #210000, processed 4669737 words, keeping 67343 word types\n",
      "2019-05-17 13:19:03,886 : INFO : PROGRESS: at sentence #220000, processed 4894637 words, keeping 68651 word types\n",
      "2019-05-17 13:19:03,924 : INFO : PROGRESS: at sentence #230000, processed 5117221 words, keeping 69912 word types\n",
      "2019-05-17 13:19:03,962 : INFO : PROGRESS: at sentence #240000, processed 5344761 words, keeping 71120 word types\n",
      "2019-05-17 13:19:03,997 : INFO : PROGRESS: at sentence #250000, processed 5558874 words, keeping 72302 word types\n",
      "2019-05-17 13:19:04,033 : INFO : PROGRESS: at sentence #260000, processed 5778770 words, keeping 73428 word types\n",
      "2019-05-17 13:19:04,069 : INFO : PROGRESS: at sentence #270000, processed 6000078 words, keeping 74718 word types\n",
      "2019-05-17 13:19:04,105 : INFO : PROGRESS: at sentence #280000, processed 6226010 words, keeping 76324 word types\n",
      "2019-05-17 13:19:04,141 : INFO : PROGRESS: at sentence #290000, processed 6449120 words, keeping 77791 word types\n",
      "2019-05-17 13:19:04,177 : INFO : PROGRESS: at sentence #300000, processed 6673746 words, keeping 79125 word types\n",
      "2019-05-17 13:19:04,214 : INFO : PROGRESS: at sentence #310000, processed 6899075 words, keeping 80435 word types\n",
      "2019-05-17 13:19:04,250 : INFO : PROGRESS: at sentence #320000, processed 7123912 words, keeping 81763 word types\n",
      "2019-05-17 13:19:04,286 : INFO : PROGRESS: at sentence #330000, processed 7345755 words, keeping 82985 word types\n",
      "2019-05-17 13:19:04,323 : INFO : PROGRESS: at sentence #340000, processed 7575223 words, keeping 84236 word types\n",
      "2019-05-17 13:19:04,359 : INFO : PROGRESS: at sentence #350000, processed 7798463 words, keeping 85382 word types\n",
      "2019-05-17 13:19:04,394 : INFO : PROGRESS: at sentence #360000, processed 8019094 words, keeping 86553 word types\n",
      "2019-05-17 13:19:04,433 : INFO : PROGRESS: at sentence #370000, processed 8246252 words, keeping 87665 word types\n",
      "2019-05-17 13:19:04,468 : INFO : PROGRESS: at sentence #380000, processed 8471464 words, keeping 88836 word types\n",
      "2019-05-17 13:19:04,505 : INFO : PROGRESS: at sentence #390000, processed 8701173 words, keeping 89865 word types\n",
      "2019-05-17 13:19:04,540 : INFO : PROGRESS: at sentence #400000, processed 8924041 words, keeping 90874 word types\n",
      "2019-05-17 13:19:04,576 : INFO : PROGRESS: at sentence #410000, processed 9145457 words, keeping 91839 word types\n",
      "2019-05-17 13:19:04,611 : INFO : PROGRESS: at sentence #420000, processed 9366524 words, keeping 92872 word types\n",
      "2019-05-17 13:19:04,647 : INFO : PROGRESS: at sentence #430000, processed 9594185 words, keeping 93893 word types\n",
      "2019-05-17 13:19:04,685 : INFO : PROGRESS: at sentence #440000, processed 9820789 words, keeping 94866 word types\n",
      "2019-05-17 13:19:04,724 : INFO : PROGRESS: at sentence #450000, processed 10044577 words, keeping 95997 word types\n",
      "2019-05-17 13:19:04,763 : INFO : PROGRESS: at sentence #460000, processed 10277317 words, keeping 97049 word types\n",
      "2019-05-17 13:19:04,802 : INFO : PROGRESS: at sentence #470000, processed 10505240 words, keeping 97894 word types\n",
      "2019-05-17 13:19:04,840 : INFO : PROGRESS: at sentence #480000, processed 10725756 words, keeping 98824 word types\n",
      "2019-05-17 13:19:04,878 : INFO : PROGRESS: at sentence #490000, processed 10952493 words, keeping 99833 word types\n",
      "2019-05-17 13:19:04,915 : INFO : PROGRESS: at sentence #500000, processed 11174042 words, keeping 100726 word types\n",
      "2019-05-17 13:19:04,953 : INFO : PROGRESS: at sentence #510000, processed 11399361 words, keeping 101660 word types\n",
      "2019-05-17 13:19:04,990 : INFO : PROGRESS: at sentence #520000, processed 11622685 words, keeping 102559 word types\n",
      "2019-05-17 13:19:05,027 : INFO : PROGRESS: at sentence #530000, processed 11847178 words, keeping 103361 word types\n",
      "2019-05-17 13:19:05,065 : INFO : PROGRESS: at sentence #540000, processed 12071804 words, keeping 104227 word types\n",
      "2019-05-17 13:19:05,101 : INFO : PROGRESS: at sentence #550000, processed 12297247 words, keeping 105095 word types\n",
      "2019-05-17 13:19:05,137 : INFO : PROGRESS: at sentence #560000, processed 12518512 words, keeping 105958 word types\n",
      "2019-05-17 13:19:05,174 : INFO : PROGRESS: at sentence #570000, processed 12747545 words, keeping 106749 word types\n",
      "2019-05-17 13:19:05,210 : INFO : PROGRESS: at sentence #580000, processed 12969078 words, keeping 107627 word types\n",
      "2019-05-17 13:19:05,245 : INFO : PROGRESS: at sentence #590000, processed 13194583 words, keeping 108463 word types\n",
      "2019-05-17 13:19:05,281 : INFO : PROGRESS: at sentence #600000, processed 13416783 words, keeping 109180 word types\n",
      "2019-05-17 13:19:05,316 : INFO : PROGRESS: at sentence #610000, processed 13637839 words, keeping 110054 word types\n",
      "2019-05-17 13:19:05,353 : INFO : PROGRESS: at sentence #620000, processed 13864196 words, keeping 110802 word types\n",
      "2019-05-17 13:19:05,389 : INFO : PROGRESS: at sentence #630000, processed 14088588 words, keeping 111573 word types\n",
      "2019-05-17 13:19:05,424 : INFO : PROGRESS: at sentence #640000, processed 14309227 words, keeping 112380 word types\n",
      "2019-05-17 13:19:05,460 : INFO : PROGRESS: at sentence #650000, processed 14534941 words, keeping 113159 word types\n",
      "2019-05-17 13:19:05,495 : INFO : PROGRESS: at sentence #660000, processed 14757724 words, keeping 113908 word types\n",
      "2019-05-17 13:19:05,532 : INFO : PROGRESS: at sentence #670000, processed 14981121 words, keeping 114606 word types\n",
      "2019-05-17 13:19:05,570 : INFO : PROGRESS: at sentence #680000, processed 15206063 words, keeping 115318 word types\n",
      "2019-05-17 13:19:05,607 : INFO : PROGRESS: at sentence #690000, processed 15428187 words, keeping 116095 word types\n",
      "2019-05-17 13:19:05,645 : INFO : PROGRESS: at sentence #700000, processed 15656856 words, keeping 116907 word types\n",
      "2019-05-17 13:19:05,682 : INFO : PROGRESS: at sentence #710000, processed 15879871 words, keeping 117561 word types\n",
      "2019-05-17 13:19:05,719 : INFO : PROGRESS: at sentence #720000, processed 16105146 words, keeping 118186 word types\n",
      "2019-05-17 13:19:05,758 : INFO : PROGRESS: at sentence #730000, processed 16331542 words, keeping 118919 word types\n",
      "2019-05-17 13:19:05,794 : INFO : PROGRESS: at sentence #740000, processed 16552584 words, keeping 119633 word types\n",
      "2019-05-17 13:19:05,830 : INFO : PROGRESS: at sentence #750000, processed 16770890 words, keeping 120260 word types\n",
      "2019-05-17 13:19:05,865 : INFO : PROGRESS: at sentence #760000, processed 16990281 words, keeping 120895 word types\n",
      "2019-05-17 13:19:05,901 : INFO : PROGRESS: at sentence #770000, processed 17217404 words, keeping 121668 word types\n",
      "2019-05-17 13:19:05,938 : INFO : PROGRESS: at sentence #780000, processed 17447550 words, keeping 122368 word types\n",
      "2019-05-17 13:19:05,974 : INFO : PROGRESS: at sentence #790000, processed 17674634 words, keeping 123031 word types\n",
      "2019-05-17 13:19:05,994 : INFO : collected 123469 word types from a corpus of 17797667 raw words and 795534 sentences\n",
      "2019-05-17 13:19:05,995 : INFO : Loading a fresh vocabulary\n",
      "2019-05-17 13:19:06,046 : INFO : min_count=40 retains 16488 unique words (13% of original 123469, drops 106981)\n",
      "2019-05-17 13:19:06,047 : INFO : min_count=40 leaves 17238582 word corpus (96% of original 17797667, drops 559085)\n",
      "2019-05-17 13:19:06,079 : INFO : deleting the raw counts dictionary of 123469 items\n",
      "2019-05-17 13:19:06,081 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-05-17 13:19:06,082 : INFO : downsampling leaves estimated 12749271 word corpus (74.0% of prior 17238582)\n",
      "2019-05-17 13:19:06,113 : INFO : estimated required memory for 16488 words and 300 dimensions: 47815200 bytes\n",
      "2019-05-17 13:19:06,114 : INFO : resetting layer weights\n",
      "2019-05-17 13:19:06,283 : INFO : training model with 10 workers on 16488 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-17 13:19:07,294 : INFO : EPOCH 1 - PROGRESS: at 11.84% examples, 1497321 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:08,301 : INFO : EPOCH 1 - PROGRESS: at 22.71% examples, 1430740 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-17 13:19:09,307 : INFO : EPOCH 1 - PROGRESS: at 33.89% examples, 1422386 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:10,308 : INFO : EPOCH 1 - PROGRESS: at 45.72% examples, 1445538 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:11,309 : INFO : EPOCH 1 - PROGRESS: at 57.58% examples, 1460576 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:12,313 : INFO : EPOCH 1 - PROGRESS: at 69.37% examples, 1467851 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:13,316 : INFO : EPOCH 1 - PROGRESS: at 81.16% examples, 1471851 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:14,318 : INFO : EPOCH 1 - PROGRESS: at 92.62% examples, 1470761 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:14,911 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-17 13:19:14,920 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-17 13:19:14,923 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-17 13:19:14,923 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-17 13:19:14,930 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-17 13:19:14,933 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-17 13:19:14,935 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-17 13:19:14,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-17 13:19:14,942 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-17 13:19:14,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-17 13:19:14,946 : INFO : EPOCH - 1 : training on 17797667 raw words (12748657 effective words) took 8.7s, 1473100 effective words/s\n",
      "2019-05-17 13:19:15,969 : INFO : EPOCH 2 - PROGRESS: at 11.66% examples, 1463206 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-17 13:19:16,983 : INFO : EPOCH 2 - PROGRESS: at 23.55% examples, 1472388 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-17 13:19:17,986 : INFO : EPOCH 2 - PROGRESS: at 35.84% examples, 1498823 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:18,990 : INFO : EPOCH 2 - PROGRESS: at 47.68% examples, 1502018 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-17 13:19:20,003 : INFO : EPOCH 2 - PROGRESS: at 59.72% examples, 1507927 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:21,011 : INFO : EPOCH 2 - PROGRESS: at 71.65% examples, 1508533 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:22,015 : INFO : EPOCH 2 - PROGRESS: at 83.45% examples, 1506577 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-17 13:19:23,016 : INFO : EPOCH 2 - PROGRESS: at 95.47% examples, 1509435 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:23,356 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-17 13:19:23,366 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-17 13:19:23,374 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-17 13:19:23,376 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-17 13:19:23,377 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-17 13:19:23,384 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-17 13:19:23,391 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-17 13:19:23,392 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-17 13:19:23,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-17 13:19:23,397 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-17 13:19:23,398 : INFO : EPOCH - 2 : training on 17797667 raw words (12748797 effective words) took 8.4s, 1510599 effective words/s\n",
      "2019-05-17 13:19:24,415 : INFO : EPOCH 3 - PROGRESS: at 11.95% examples, 1507898 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:25,423 : INFO : EPOCH 3 - PROGRESS: at 24.28% examples, 1527820 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:26,427 : INFO : EPOCH 3 - PROGRESS: at 36.51% examples, 1532895 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:27,445 : INFO : EPOCH 3 - PROGRESS: at 48.11% examples, 1514752 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:28,449 : INFO : EPOCH 3 - PROGRESS: at 59.06% examples, 1492558 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:29,458 : INFO : EPOCH 3 - PROGRESS: at 70.99% examples, 1495263 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:30,469 : INFO : EPOCH 3 - PROGRESS: at 82.99% examples, 1497886 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:31,479 : INFO : EPOCH 3 - PROGRESS: at 95.13% examples, 1502032 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:31,840 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-17 13:19:31,841 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-17 13:19:31,851 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-17 13:19:31,852 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-17 13:19:31,853 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-17 13:19:31,857 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-17 13:19:31,865 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-17 13:19:31,868 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-17 13:19:31,869 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-17 13:19:31,871 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-17 13:19:31,872 : INFO : EPOCH - 3 : training on 17797667 raw words (12749434 effective words) took 8.5s, 1506503 effective words/s\n",
      "2019-05-17 13:19:32,882 : INFO : EPOCH 4 - PROGRESS: at 11.84% examples, 1499733 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:33,888 : INFO : EPOCH 4 - PROGRESS: at 23.93% examples, 1510950 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:34,890 : INFO : EPOCH 4 - PROGRESS: at 36.17% examples, 1523299 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:35,896 : INFO : EPOCH 4 - PROGRESS: at 48.12% examples, 1522515 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-17 13:19:36,899 : INFO : EPOCH 4 - PROGRESS: at 60.13% examples, 1526174 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:37,905 : INFO : EPOCH 4 - PROGRESS: at 72.32% examples, 1530214 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:38,915 : INFO : EPOCH 4 - PROGRESS: at 84.39% examples, 1529035 words/s, in_qsize 20, out_qsize 2\n",
      "2019-05-17 13:19:39,917 : INFO : EPOCH 4 - PROGRESS: at 95.54% examples, 1514608 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:40,305 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-17 13:19:40,308 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-17 13:19:40,309 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-17 13:19:40,310 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-17 13:19:40,311 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-17 13:19:40,312 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-17 13:19:40,320 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-17 13:19:40,327 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-17 13:19:40,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-17 13:19:40,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-17 13:19:40,329 : INFO : EPOCH - 4 : training on 17797667 raw words (12750097 effective words) took 8.4s, 1509148 effective words/s\n",
      "2019-05-17 13:19:41,344 : INFO : EPOCH 5 - PROGRESS: at 11.49% examples, 1450708 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:42,351 : INFO : EPOCH 5 - PROGRESS: at 23.54% examples, 1481963 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:43,361 : INFO : EPOCH 5 - PROGRESS: at 35.67% examples, 1495249 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:44,362 : INFO : EPOCH 5 - PROGRESS: at 47.67% examples, 1505236 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-17 13:19:45,366 : INFO : EPOCH 5 - PROGRESS: at 59.23% examples, 1500594 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:46,377 : INFO : EPOCH 5 - PROGRESS: at 71.16% examples, 1501710 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-17 13:19:47,381 : INFO : EPOCH 5 - PROGRESS: at 82.93% examples, 1500724 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:48,384 : INFO : EPOCH 5 - PROGRESS: at 94.83% examples, 1502059 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-17 13:19:48,776 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-17 13:19:48,791 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-17 13:19:48,792 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-17 13:19:48,793 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-17 13:19:48,801 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-17 13:19:48,802 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-17 13:19:48,804 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-17 13:19:48,817 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-17 13:19:48,818 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-17 13:19:48,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-17 13:19:48,820 : INFO : EPOCH - 5 : training on 17797667 raw words (12749920 effective words) took 8.5s, 1503231 effective words/s\n",
      "2019-05-17 13:19:48,821 : INFO : training on a 88988335 raw words (63746905 effective words) took 42.5s, 1498631 effective words/s\n",
      "2019-05-17 13:19:48,821 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-05-17 13:19:48,973 : INFO : saving Word2Vec object under ../data/300features_40minwords_10context, separately None\n",
      "2019-05-17 13:19:48,974 : INFO : not storing attribute vectors_norm\n",
      "2019-05-17 13:19:48,975 : INFO : not storing attribute cum_table\n",
      "2019-05-17 13:19:48,975 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-17 13:19:49,274 : INFO : saved ../data/300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 10       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# Calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model to use later\n",
    "model.save('../data/300features_40minwords_10context')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hovanes/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:730: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['man', 'woman', 'child', 'kitchen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model knows that men, women, and children are more closely related than kitchens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6192024350166321),\n",
       " ('lady', 0.5974243879318237),\n",
       " ('lad', 0.5832437872886658),\n",
       " ('soldier', 0.537463903427124),\n",
       " ('monk', 0.523860514163971),\n",
       " ('guy', 0.5204955339431763),\n",
       " ('businessman', 0.5197274684906006),\n",
       " ('farmer', 0.5150461196899414),\n",
       " ('men', 0.5107800960540771),\n",
       " ('millionaire', 0.5076751708984375)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7725614309310913),\n",
       " ('atrocious', 0.7263244986534119),\n",
       " ('horrible', 0.7250977754592896),\n",
       " ('dreadful', 0.7105594873428345),\n",
       " ('abysmal', 0.705592691898346),\n",
       " ('appalling', 0.6716033220291138),\n",
       " ('horrendous', 0.6705040335655212),\n",
       " ('horrid', 0.6510659456253052),\n",
       " ('lousy', 0.6157515048980713),\n",
       " ('bad', 0.5928584337234497)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('awful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the minimum word count to 40 gave me a total vocabulary of 16,488 words with 300 features each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16488, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom function to average all of the word vectors in a given paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. Converting it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # Loop over each word in the review and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom function to get the average feature vector for each of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Calculating the average feature vector for each review and returning a 2D numpy array \n",
    "    \n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    " \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    " \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "\n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(f\"Review %d of %d\" % (counter, len(reviews)))\n",
    " \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "\n",
    "       # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the functions above to calculate the average feature vectors for the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/labeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/testData.tsv', header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hovanes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vectors for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "print(\"Creating average feature vectors for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the cleaned training and test reviews for later use in future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleantrainreviews.txt', 'wb') as fp:\n",
    "    pickle.dump(clean_train_reviews, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cleantestreviews.txt', 'wb') as fp:\n",
    "    pickle.dump(clean_test_reviews, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to try out some baseline models, I'll start off with RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainDataVecs, train.sentiment, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a Random Forest to the labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8393023980068514\n"
     ]
    }
   ],
   "source": [
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2523,  463],\n",
       "       [ 569, 2695]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, RandomForest overfits on the training data, but doesn't do too bad on the test data. It is obviously much better than chance, but there is still a lot of room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel2 = rf.fit(trainDataVecs, train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rfmodel2.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={'id':test['id'], 'sentiment':result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('../data/Word2Vec_AverageVectors.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to try out a Naive Bayes Multinomial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cc2674e7807d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    608\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    609\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18750, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
